{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import datetime\n",
    "import os\n",
    "from prefetch_generator import BackgroundGenerator, background,__doc__\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRINT VERSION!!!\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dataset\n",
    "### I'm using the amazon food reviews dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('~/Data_Science/tests/reviews.csv')\n",
    "train = train[['Summary','Text']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568454.000000\n",
       "mean         81.005522\n",
       "std          80.807102\n",
       "min           2.000000\n",
       "25%          33.000000\n",
       "50%          57.000000\n",
       "75%          99.000000\n",
       "max        3525.000000\n",
       "Name: text_length, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_length'] = train['Text'].str.count(' ')\n",
    "train['text_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568427.000000\n",
       "mean          3.128462\n",
       "std           2.619420\n",
       "min           0.000000\n",
       "25%           1.000000\n",
       "50%           3.000000\n",
       "75%           4.000000\n",
       "max          41.000000\n",
       "Name: summary_length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['summary_length'] = train['Summary'].str.count(' ')\n",
    "train['summary_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>summary_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>48</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>30</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>98</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   text_length  summary_length  \n",
       "0           48             3.0  \n",
       "1           30             2.0  \n",
       "2           98             3.0  \n",
       "3           42             1.0  \n",
       "4           29             1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bounding data lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of weirdness in test/train set\n",
    "\n",
    "train = train[train['summary_length']>=2].reset_index(drop=True)\n",
    "train = train[train['summary_length']<=20].reset_index(drop=True)\n",
    "train = train[train['text_length']<=100].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276719, 4)\n",
      "                                         Summary  \\\n",
      "0                          Good Quality Dog Food   \n",
      "1                              Not as Advertised   \n",
      "2                          \"Delight\" says it all   \n",
      "3  Great!  Just as good as the expensive brands!   \n",
      "4                         Wonderful, tasty taffy   \n",
      "\n",
      "                                                Text  text_length  \\\n",
      "0  I have bought several of the Vitality canned d...           48   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...           30   \n",
      "2  This is a confection that has been around a fe...           98   \n",
      "3  This saltwater taffy had great flavors and was...           52   \n",
      "4  This taffy is so good.  It is very soft and ch...           27   \n",
      "\n",
      "   summary_length  \n",
      "0             3.0  \n",
      "1             2.0  \n",
      "2             3.0  \n",
      "3             8.0  \n",
      "4             2.0  \n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning data and making and saving test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_lower'] = train['Text'].str.lower()\n",
    "train['text_no_punctuation'] = train['text_lower'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding \"_start_\" and \"_end_\" delimeters to summary this tells the model where to start\n",
    "\n",
    "train['summary_lower'] = train[\"Summary\"].str.lower()\n",
    "train['summary_no_punctuation'] =  '_start_' + ' ' +train['summary_lower'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataset and reset index\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "test = train[0:100]\n",
    "train = train[100:]\n",
    "\n",
    "test.to_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## playing with max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting max features and max len for text and summarty for model\n",
    "\n",
    "max_features1 = 100000\n",
    "maxlen1 = 100\n",
    "\n",
    "max_features2 = 100000\n",
    "maxlen2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making tokenizers and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok1 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features1) \n",
    "tok1.fit_on_texts(list(train['text_no_punctuation'].astype(str))) #fit to cleaned text\n",
    "tf_train_text =tok1.texts_to_sequences(list(train['text_no_punctuation'].astype(str)))\n",
    "tf_train_text =tf.keras.preprocessing.sequence.pad_sequences(tf_train_text, maxlen=maxlen1) #let's execute pad step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tokenizer for scoring later on\n",
    "\n",
    "tokenizer1_json = tok1.to_json()\n",
    "with io.open('tok1.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer1_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the processing has to be done for both \n",
    "#two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = tf.keras.preprocessing.text.Tokenizer(num_words=max_features2, filters = '*') \n",
    "tok2.fit_on_texts(list(train['summary_no_punctuation'].astype(str))) #fit to cleaned text\n",
    "tf_train_summary = tok2.texts_to_sequences(list(train['summary_no_punctuation'].astype(str)))\n",
    "tf_train_summary = tf.keras.preprocessing.sequence.pad_sequences(tf_train_summary, maxlen=maxlen2, padding ='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2_json = tok2.to_json()\n",
    "with io.open('tok2.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer2_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting dimensions and getting the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of decoder input: (276619, 19)\n",
      "Shape of decoder target: (276619, 19)\n",
      "Shape of encoder input: (276619, 100)\n"
     ]
    }
   ],
   "source": [
    "vectorized_summary = tf_train_summary\n",
    "# For Decoder Input, you don't need the last word as that is only for prediction\n",
    "# when we are training using Teacher Forcing.\n",
    "decoder_input_data = vectorized_summary[:, :-1]\n",
    "\n",
    "# Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "decoder_target_data = vectorized_summary[:, 1:]\n",
    "\n",
    "print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "\n",
    "vectorized_text = tf_train_text\n",
    "# Encoder input is simply the body of the text\n",
    "encoder_input_data = vectorized_text\n",
    "doc_length = encoder_input_data.shape[1]\n",
    "print(f'Shape of encoder input: {encoder_input_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting size of vocabulary encoder and decoder\n",
    "\n",
    "vocab_size_encoder = len(tok1.word_index) + 1 \n",
    "vocab_size_decoder = len(tok2.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set latent dimension for embedding and hidden units\n",
    "\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Preparing GloVe\n",
    "\n",
    "GLOVE_DIR = \"/home/tiana/Data_Science/tests/GloVe\"\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(latent_dim)))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build embedding weights matrix for text\n",
    "\n",
    "embedding_matrix = np.zeros((len(tok1.word_index) + 1, latent_dim))\n",
    "for word, i in tok1.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### Encoder Model ####\n",
    "\n",
    "#setting Encoder Input\n",
    "encoder_inputs = tf.keras.Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# GloVe Embeding for encoder\n",
    "x = tf.keras.layers.Embedding(vocab_size_encoder, \n",
    "                              latent_dim, \n",
    "                              name='Body-Word-Embedding',\n",
    "                              weights=[embedding_matrix],\n",
    "                              mask_zero=False, \n",
    "                              trainable=False)(encoder_inputs)\n",
    "\n",
    "#Batch normalization is used so that the distribution of the inputs \n",
    "#to a specific layer doesn't change over time\n",
    "x = tf.keras.layers.BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state\n",
    "_, state_h = tf.keras.layers.GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Set the encoder as a separate entity so we can encode without decoding if desired\n",
    "encoder_model = tf.keras.Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Embedding For Decoder, not GloVe \n",
    "dec_emb = tf.keras.layers.Embedding(vocab_size_decoder, \n",
    "                                    latent_dim, \n",
    "                                    name='Decoder-Word-Embedding',\n",
    "                                    mask_zero=False, )(decoder_inputs)\n",
    "\n",
    "#batch normalization\n",
    "dec_bn = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "#the decoder \"decodes\" the encoder out\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = tf.keras.layers.BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size_decoder, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "seq2seq_Model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "#parallelize data on N GPUs if desired\n",
    "#seq2seq_Model = tf.keras.utils.multi_gpu_model(seq2seq_Model, gpus=N)\n",
    "\n",
    "seq2seq_Model.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), \n",
    "                      loss='sparse_categorical_crossentropy', \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 100)    2876300     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 100)    400         Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 100)          11202100    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 100),  60600       Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 100)    400         Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 28763)  2905063     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 17,044,863\n",
      "Trainable params: 5,903,163\n",
      "Non-trainable params: 11,141,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit(x=None, y=None, \n",
    "    batch_size=None, \n",
    "    epochs=1, verbose=1, \n",
    "    callbacks=None, \n",
    "    validation_split=0.0, \n",
    "    validation_data=None, \n",
    "    shuffle=True, \n",
    "    class_weight=None, \n",
    "    sample_weight=None, \n",
    "    initial_epoch=0, \n",
    "    steps_per_epoch=None, \n",
    "    validation_steps=None, \n",
    "    validation_freq=1, \n",
    "    max_queue_size=10, \n",
    "    workers=1, \n",
    "    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_generator(generator, \n",
    "              steps_per_epoch=None, \n",
    "              epochs=1, verbose=1, \n",
    "              callbacks=None, \n",
    "              validation_data=None, \n",
    "              validation_steps=None, \n",
    "              validation_freq=1, \n",
    "              class_weight=None, \n",
    "              max_queue_size=10, \n",
    "              workers=1, \n",
    "              use_multiprocessing=False, \n",
    "              shuffle=True, \n",
    "              initial_epoch=0)\n",
    "\n",
    "The use of keras.utils.Sequence guarantees the ordering and guarantees the single use of every input \n",
    "per epoch when using use_multiprocessing=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#spencer generator\n",
    "\n",
    "features = [encoder_input_data, decoder_input_data]\n",
    "target = np.expand_dims(decoder_target_data, -1)\n",
    "batch_size = 32\n",
    "num_samples = ____\n",
    "\n",
    "def S_generator(features, target, batch_size):\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            for x,y in zip(features, target):\n",
    "                X_train = features[i]\n",
    "                y_train = target[j]\n",
    "                prefetched = dataset.prefetch(2)\n",
    "            yield prefetched.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "\n",
    "            # For each example\n",
    "            for batch_sample in batch_samples:\n",
    "                # Load image (X)\n",
    "                filename = './common_filepath/'+batch_sample[0]\n",
    "                image = mpimg.imread(filename)\n",
    "                # Read label (y)\n",
    "                y = batch_sample[1]\n",
    "                # Add example to arrays\n",
    "                X_train.append(image)\n",
    "                y_train.append(y)\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, y_train\n",
    "\n",
    "# Import list of train and validation data (image filenames and image labels)\n",
    "# Note this is not valid code.\n",
    "train_samples = ...\n",
    "validation_samples = ...\n",
    "\n",
    "# Create generator\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "#######################\n",
    "# Use generator to train neural network in Keras\n",
    "#######################\n",
    "\n",
    "# Create model in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "# Fit model using generator\n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_plain(features, labels, batch_size):\n",
    "\n",
    "    batch_features = np.zeros((batch_size, IMG_SIZE, IMG_SIZE, NUM_CHANNELS), dtype=np.float64)\n",
    "    batch_labels = np.zeros((batch_size, NUM_KEYPOINTS * 2), dtype=np.float64)\n",
    "\n",
    "    while True:\n",
    "        steps = len(batch_features) // batch_size\n",
    "        for i in range(steps):\n",
    "            for j in range(batch_size):\n",
    "                batch_features[j] = features[(i*batch_size)+j]\n",
    "                batch_labels[j] = labels[(i*batch_size)+j]\n",
    "\n",
    "            yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", header=0, delimiter=\"\\t\", quoting=3, encoding=\"utf-8\")\n",
    "y = data.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "\n",
    "def data_genereator(data, batch_size):\n",
    "        num_rows = int(data.shape[0])\n",
    "        # Initialize a counter\n",
    "        counter = 0\n",
    "        while True:\n",
    "            for content, label in zip(data['content'], data['label']):\n",
    "                X_train[counter%batch_size] = transform(content)\n",
    "                y_train[counter%batch_size] = np.asarray(label)\n",
    "                counter = counter + 1\n",
    "                if(counter%batch_size == 0):\n",
    "                    yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_queue(generator, max_q_size=10,\n",
    "                    wait_time=0.05, nb_worker=1):\n",
    "    '''Builds a threading queue out of a data generator.\n",
    "    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n",
    "    '''\n",
    "    q = queue.Queue()\n",
    "    _stop = threading.Event()\n",
    "\n",
    "    def data_generator_task():\n",
    "        while not _stop.is_set():\n",
    "            try:\n",
    "                if q.qsize() < max_q_size:\n",
    "                    try:\n",
    "                        generator_output = next(generator)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    q.put(generator_output)\n",
    "                else:\n",
    "                    time.sleep(wait_time)\n",
    "            except Exception:\n",
    "                _stop.set()\n",
    "                raise\n",
    "\n",
    "    generator_threads = [threading.Thread(target=data_generator_task)\n",
    "                         for _ in range(nb_worker)]\n",
    "\n",
    "    for thread in generator_threads:\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "\n",
    "    return q, _stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[19:19, 3/2/2020] Spencer Thomas Hoffman: 'How ds.repeat() works\n",
    "\n",
    "As soon as all the entries are read from the dataset and you try to read the next element, the dataset will throw an error. That's where ds.repeat() comes into play. It will re-initialize the dataset, making it again like this:\n",
    "\n",
    "[1,2,3] <= [4,5,6]'\n",
    "[19:20, 3/2/2020] Spencer Thomas Hoffman: also repeat() does not work in place; you have to assign to a new var\n",
    "[19:20, 3/2/2020] Spencer Thomas Hoffman: Better explanation:\n",
    "[19:20, 3/2/2020] Spencer Thomas Hoffman: 'As we know, each epoch in the training process of a model takes in the whole dataset and breaks it into batches. This happens on every epoch. Suppose, we have a dataset with 100 samples. On every epoch, the 100 samples are broken into 5 batches ( of 20 each ) for feeding them to the model. But, if I have to train the model for say 5 epochs then, I need to repeat the dataset 5 times. Meaning, the total elements in the repeated dataset will have 500 samples ( 100 samples multipled 5 times ).\n",
    "\n",
    "Now, this job is done by the tf.data.Dataset.repeat() method. Usually we pass the num_epochs argument to the method.\n",
    "\n",
    "The iterator.get_next() is just a way of getting the next batch of data from the tf.data.Dataset. You are iterating the dataset batch by batch.\n",
    "\n",
    "That's the difference. The tf.data.Dataset.repeat() repeats the samples in the dataset whereas iterator.get_next() one-by-one fetches the data in the form of batches.'\n",
    "[19:21, 3/2/2020] Spencer Thomas Hoffman: IMO this actually should work\n",
    "[19:21, 3/2/2020] Spencer Thomas Hoffman: another example\n",
    "[19:21, 3/2/2020] Spencer Thomas Hoffman: dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "[19:22, 3/2/2020] Spencer Thomas Hoffman: iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "[19:24, 3/2/2020] Spencer Thomas Hoffman: I tried it with an array\n",
    "[19:24, 3/2/2020] Spencer Thomas Hoffman: it worked\n",
    "[19:24, 3/2/2020] Spencer Thomas Hoffman: In [54]: for x in dataset2.repeat(3):\n",
    "    ...:     print(x)\n",
    "    ...:\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n",
    "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
    "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
    "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
    "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n",
    "[19:24, 3/2/2020] Spencer Thomas Hoffman: Simple test\n",
    "[19:25, 3/2/2020] Tiana Cornelius: hmmm - taking this in rn\n",
    "[19:25, 3/2/2020] Spencer Thomas Hoffman: My code:\n",
    "[19:25, 3/2/2020] Spencer Thomas Hoffman: dataset = tf.data.Dataset.from_tensor_slices([[0,1,2],[3,4,5],[6,7,8],[9,10,11]])\n",
    "[19:25, 3/2/2020] Spencer Thomas Hoffman: dataset2 = dataset.prefetch(2)\n",
    "[19:25, 3/2/2020] Spencer Thomas Hoffman: for x in dataset2.repeat(3):\n",
    "    ...:     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
      "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n",
      "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
      "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n",
      "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
      "tf.Tensor([3 4 5], shape=(3,), dtype=int32)\n",
      "tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n",
      "tf.Tensor([ 9 10 11], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([[0,1,2],[3,4,5],[6,7,8],[9,10,11]])\n",
    "dataset2 = dataset.prefetch(2)\n",
    "for x in dataset2.repeat(3):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for reusing data we could take the data and put it back into a dataframe or dictionary or something and then shuffle it and put it back into the inddpendent arrays\n",
    "\n",
    "or what is this Keras sequence thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "encoder_input_data = [[1,2],[4,5]]\n",
    "decoder_input_data = [[7,8],[10,11]]\n",
    "y_t = [[12, 13, 14], [15, 16, 17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "if BUFFER_SIZE % BATCH_SIZE != 0:\n",
    "    parallel_steps = BUFFER_SIZE // BATCH_SIZE + 1\n",
    "else:\n",
    "    parallel_steps = BUFFER_SIZE // BATCH_SIZE\n",
    "\n",
    "# This `fit` call will be distributed on 2 GPUs.\n",
    "# Since the batch size is 64, each GPU will process 32 samples.\n",
    "parallel_model.fit(train_dataset, epochs=10, steps_per_epoch = parallel_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.repeat(a = np_array_2d, repeats = 2, axis = 0)\n",
    "\n",
    "#X_train = [encoder_input_data, decoder_input_data]\n",
    "X_enc = np.repeat(a=encoder_input_data, repeats=50, axis=0) # array with shape (276619, 100)\n",
    "X_dec = np.repeat(a=decoder_input_data, repeats=50, axis=0) # array with shape (276619, 19)\n",
    "y_t = np.expand_dims(np.repeat(a=decoder_target_data, repeats=50, axis=0), -1) #array with shape (276619, 19, 1)\n",
    "#batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 6.68 µs\n",
      "/------------------------------------------ Progress Bar ------------------------------------------\\\n",
      "! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n"
     ]
    }
   ],
   "source": [
    "def iterate_minibatches(n_batches, rows=500):\n",
    "    counter = 0\n",
    "    for b_i in range(n_batches):\n",
    "        time.sleep(0.1) #here it could read file or SQL-get or do some math\n",
    "        Xe = X_enc[counter:(counter+rows)]\n",
    "        Xd = X_dec[counter:(counter+rows)]\n",
    "        y_train = y_t[counter:(counter+rows)]\n",
    "        X_train = [Xe,Xd] \n",
    "        counter = counter + rows\n",
    "        yield X_train,y_train\n",
    "%time\n",
    "\n",
    "print('/'+'-'*42+' Progress Bar ' + '-'*42 + '\\\\')\n",
    "\n",
    "for b_x,b_y in iterate_minibatches(50):\n",
    "    #training\n",
    "    time.sleep(0.1)#you guessed it\n",
    "    print('!',end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generatorClass(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.lock = threading.Lock()   #Set self.lock\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with self.lock:                #Use self.lock\n",
    "            batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "            return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Sequence):\n",
    "    \"\"\" Implements the Sequence iterator for a sequence of tokens \"\"\"\n",
    "    def __init__(self,\n",
    "                 X_enc, \n",
    "                 X_def,\n",
    "                 y_t,\n",
    "                 look_back=1,\n",
    "                 batch_size=5):\n",
    "        #self.tokens_sequence = tokens_sequence - X_enc, X_dec\n",
    "        #self.num_classes = num_classes - y_t\n",
    "        self.look_back = look_back\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index):\n",
    "        \"\"\"Gets batch at position `index`.\n",
    "        # Arguments\n",
    "            index: position of the batch in the Sequence.\n",
    "        # Returns\n",
    "            A batch\n",
    "        \"\"\"\n",
    "        begin = index*self.batch_size\n",
    "        if (index + 1) == self.__len__():\n",
    "            # In the last batch we add all the remaining data\n",
    "            end = len(self.tokens_sequence) - self.look_back\n",
    "        else:\n",
    "            end = (index+1)*self.batch_size\n",
    "        x = np.array([self.tokens_sequence[i:i+self.look_back]\n",
    "                      for i in range(begin, end)])\n",
    "        y = np.array(self.tokens_sequence[\n",
    "            (begin + self.look_back):(end + self.look_back)])\n",
    "        y = y[:, np.newaxis]\n",
    "        y = to_categorical(y, self.num_classes)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batch in the Sequence.\n",
    "        # Returns\n",
    "            The number of batches in the Sequence.\n",
    "        \"\"\"\n",
    "        # We round to the floor, therefore we \"skip\" some data.\n",
    "        # As a work around, the remainder is added to the last batch\n",
    "        return int(np.floor(\n",
    "            (len(self.tokens_sequence) - self.look_back) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Method called at the end of every epoch.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276619"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(encoder_input_data)\n",
    "val_split = int(np.floor(data_len*.15))\n",
    "train_split = int(np.floor(data_len*.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41492"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235126"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating into train and validation data\n",
    "\n",
    "X_enc_train = encoder_input_data[0:train_split]\n",
    "X_dec_train = decoder_input_data[0:train_split]\n",
    "y_t_train = np.expand_dims(decoder_target_data, -1)[0:train_split]\n",
    "\n",
    "X_enc_val = encoder_input_data[-val_split:-1]\n",
    "X_dec_val = decoder_input_data[-val_split:-1]\n",
    "y_t_val = np.expand_dims(decoder_target_data, -1)[-val_split:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variables for generator\n",
    "#X_enc = encoder_input_data # array with shape (276619, 100)\n",
    "#X_dec = decoder_input_data # array with shape (276619, 19)\n",
    "#y_t = np.expand_dims(decoder_target_data, -1) #array with shape (276619, 19, 1)\n",
    "#idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generatorClass(Sequence):\n",
    "\n",
    "    def __init__(self, X_enc, X_dec, y_t, batch_size):\n",
    "        self.X_enc = X_enc\n",
    "        self.X_dec = X_dec\n",
    "        self.y_t = y_t\n",
    "        self.batch_size = batch_size\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X_enc) / float(self.batch_size)))\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        with self.lock:\n",
    "            batch_index1 = idx * self.batch_size\n",
    "            batch_index2 = (idx + 1) * self.batch_size\n",
    "            batch_Xe = self.X_enc[batch_index1:batch_index2]\n",
    "            batch_Xd = self.X_dec[batch_index1:batch_index2]\n",
    "            batch_y = self.y_t[batch_index1:batch_index2]\n",
    "            batch_X = [batch_Xe, batch_Xd]\n",
    "\n",
    "            return batch_X, batch_y\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Method called at the end of every epoch.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 2.6096 - accuracy: 0.7494\n",
      "Epoch 00001: val_loss improved from inf to 8.83589, saving model to /tmp/weights.01-8.84.hdf5\n",
      "395/395 [==============================] - 150s 380ms/step - loss: 2.6064 - accuracy: 0.7495 - val_loss: 8.8359 - val_accuracy: 0.4849\n",
      "Epoch 2/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 1.2534 - accuracy: 0.7989\n",
      "Epoch 00002: val_loss improved from 8.83589 to 1.39935, saving model to /tmp/weights.02-1.40.hdf5\n",
      "395/395 [==============================] - 147s 371ms/step - loss: 1.2531 - accuracy: 0.7989 - val_loss: 1.3994 - val_accuracy: 0.8041\n",
      "Epoch 3/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 1.0729 - accuracy: 0.8139\n",
      "Epoch 00003: val_loss improved from 1.39935 to 1.11244, saving model to /tmp/weights.03-1.11.hdf5\n",
      "395/395 [==============================] - 146s 370ms/step - loss: 1.0728 - accuracy: 0.8139 - val_loss: 1.1124 - val_accuracy: 0.8135\n",
      "Epoch 4/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.9741 - accuracy: 0.8230\n",
      "Epoch 00004: val_loss improved from 1.11244 to 1.06726, saving model to /tmp/weights.04-1.07.hdf5\n",
      "395/395 [==============================] - 147s 371ms/step - loss: 0.9740 - accuracy: 0.8230 - val_loss: 1.0673 - val_accuracy: 0.8188\n",
      "Epoch 5/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.9038 - accuracy: 0.8307\n",
      "Epoch 00005: val_loss improved from 1.06726 to 1.04713, saving model to /tmp/weights.05-1.05.hdf5\n",
      "395/395 [==============================] - 147s 371ms/step - loss: 0.9038 - accuracy: 0.8307 - val_loss: 1.0471 - val_accuracy: 0.8225\n",
      "Epoch 6/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.8136 - accuracy: 0.8420\n",
      "Epoch 00007: val_loss improved from 1.03785 to 1.03630, saving model to /tmp/weights.07-1.04.hdf5\n",
      "395/395 [==============================] - 147s 371ms/step - loss: 0.8135 - accuracy: 0.8420 - val_loss: 1.0363 - val_accuracy: 0.8270\n",
      "Epoch 8/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.7865 - accuracy: 0.8460\n",
      "Epoch 00008: val_loss improved from 1.03630 to 1.03600, saving model to /tmp/weights.08-1.04.hdf5\n",
      "395/395 [==============================] - 147s 372ms/step - loss: 0.7865 - accuracy: 0.8460 - val_loss: 1.0360 - val_accuracy: 0.8280\n",
      "Epoch 9/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.7670 - accuracy: 0.8489\n",
      "Epoch 00009: val_loss improved from 1.03600 to 1.03517, saving model to /tmp/weights.09-1.04.hdf5\n",
      "395/395 [==============================] - 147s 372ms/step - loss: 0.7670 - accuracy: 0.8489 - val_loss: 1.0352 - val_accuracy: 0.8295\n",
      "Epoch 10/10\n",
      "394/395 [============================>.] - ETA: 0s - loss: 0.7483 - accuracy: 0.8520\n",
      "Epoch 00010: val_loss did not improve from 1.03517\n",
      "395/395 [==============================] - 146s 371ms/step - loss: 0.7483 - accuracy: 0.8520 - val_loss: 1.0372 - val_accuracy: 0.8305\n"
     ]
    }
   ],
   "source": [
    "#tensorboard\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "#checkpoints\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/tmp/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "    monitor = 'val_loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    sav_freq='epoch')\n",
    "\n",
    "#early_stopping\n",
    "#early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  \n",
    "#                                              patience=10, verbose=1, mode='auto', \n",
    "#                                              restore_best_weights=True)\n",
    "\n",
    "#model\n",
    "#X_train = [encoder_input_data, decoder_input_data]\n",
    "#y_train = np.expand_dims(decoder_target_data, -1)\n",
    "epochs = 10\n",
    "batch_size = 700\n",
    "idx = 0\n",
    "gen_instance = generatorClass(X_enc_train, X_dec_train, y_t_train, batch_size)\n",
    "val_instance = generatorClass(X_enc_val, X_dec_val, y_t_val, batch_size)\n",
    "#n_batches = (int(encoder_input_data.shape[0]) // batch_size) * epochs\n",
    "history = seq2seq_Model.fit_generator(generator = gen_instance,\n",
    "                            epochs=epochs ,  \n",
    "                            max_queue_size=50, \n",
    "                            validation_data = val_instance,\n",
    "                            validation_freq=1,\n",
    "                            steps_per_epoch = int(encoder_input_data.shape[0]) // batch_size,\n",
    "                            callbacks=[tensorboard_callback, checkpointer], #early_stop],\n",
    "                            use_multiprocessing=True,\n",
    "                            workers=7) \n",
    "\n",
    "#save final model\n",
    "#seq2seq_Model.save('200_epochs_amazon_glove.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save final model\n",
    "seq2seq_Model.save('10_epochs_amazon_glove.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "seq2seq_Model = tf.keras.models.load_model('200_epochs_amazon_glove.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the tokenizers\n",
    "\n",
    "with open('tok1.json') as f:\n",
    "    data = json.load(f)\n",
    "    tok1 = tokenizer_from_json(data)\n",
    "    \n",
    "with open('tok2.json') as f:\n",
    "    data = json.load(f)\n",
    "    tok2 = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>text_lower</th>\n",
       "      <th>text_no_punctuation</th>\n",
       "      <th>summary_lower</th>\n",
       "      <th>summary_no_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Great cookie...a little pricey</td>\n",
       "      <td>One of the things that I missed the most when ...</td>\n",
       "      <td>100</td>\n",
       "      <td>3.0</td>\n",
       "      <td>one of the things that i missed the most when ...</td>\n",
       "      <td>one of the things that i missed the most when ...</td>\n",
       "      <td>great cookie...a little pricey</td>\n",
       "      <td>_start_ great cookiea little pricey _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Always the right formula</td>\n",
       "      <td>I trust this brand--the flavors are blended ju...</td>\n",
       "      <td>20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>i trust this brand--the flavors are blended ju...</td>\n",
       "      <td>i trust this brandthe flavors are blended just...</td>\n",
       "      <td>always the right formula</td>\n",
       "      <td>_start_ always the right formula _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the real taste of an Italian coffee</td>\n",
       "      <td>This is my favorite one. I'm Italian and I use...</td>\n",
       "      <td>50</td>\n",
       "      <td>6.0</td>\n",
       "      <td>this is my favorite one. i'm italian and i use...</td>\n",
       "      <td>this is my favorite one im italian and i used ...</td>\n",
       "      <td>the real taste of an italian coffee</td>\n",
       "      <td>_start_ the real taste of an italian coffee _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Love the bags</td>\n",
       "      <td>No matter what kind of coffee that I brew, my ...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no matter what kind of coffee that i brew, my ...</td>\n",
       "      <td>no matter what kind of coffee that i brew my h...</td>\n",
       "      <td>love the bags</td>\n",
       "      <td>_start_ love the bags _end_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Unique flavor, excellent healthy snack</td>\n",
       "      <td>We love Garden of Eatin' chips. Our favorites ...</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>we love garden of eatin' chips. our favorites ...</td>\n",
       "      <td>we love garden of eatin chips our favorites va...</td>\n",
       "      <td>unique flavor, excellent healthy snack</td>\n",
       "      <td>_start_ unique flavor excellent healthy snack ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Summary  \\\n",
       "0          Great cookie...a little pricey   \n",
       "1                Always the right formula   \n",
       "2     the real taste of an Italian coffee   \n",
       "3                           Love the bags   \n",
       "4  Unique flavor, excellent healthy snack   \n",
       "\n",
       "                                                Text  text_length  \\\n",
       "0  One of the things that I missed the most when ...          100   \n",
       "1  I trust this brand--the flavors are blended ju...           20   \n",
       "2  This is my favorite one. I'm Italian and I use...           50   \n",
       "3  No matter what kind of coffee that I brew, my ...           32   \n",
       "4  We love Garden of Eatin' chips. Our favorites ...           42   \n",
       "\n",
       "   summary_length                                         text_lower  \\\n",
       "0             3.0  one of the things that i missed the most when ...   \n",
       "1             3.0  i trust this brand--the flavors are blended ju...   \n",
       "2             6.0  this is my favorite one. i'm italian and i use...   \n",
       "3             2.0  no matter what kind of coffee that i brew, my ...   \n",
       "4             4.0  we love garden of eatin' chips. our favorites ...   \n",
       "\n",
       "                                 text_no_punctuation  \\\n",
       "0  one of the things that i missed the most when ...   \n",
       "1  i trust this brandthe flavors are blended just...   \n",
       "2  this is my favorite one im italian and i used ...   \n",
       "3  no matter what kind of coffee that i brew my h...   \n",
       "4  we love garden of eatin chips our favorites va...   \n",
       "\n",
       "                            summary_lower  \\\n",
       "0          great cookie...a little pricey   \n",
       "1                always the right formula   \n",
       "2     the real taste of an italian coffee   \n",
       "3                           love the bags   \n",
       "4  unique flavor, excellent healthy snack   \n",
       "\n",
       "                              summary_no_punctuation  \n",
       "0          _start_ great cookiea little pricey _end_  \n",
       "1             _start_ always the right formula _end_  \n",
       "2  _start_ the real taste of an italian coffee _end_  \n",
       "3                        _start_ love the bags _end_  \n",
       "4  _start_ unique flavor excellent healthy snack ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at test set\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they are like most chinese fast food place and low price eatery fortune cookiesbr br many have motivating messages perfect for company functions but lack soulful messages often found in better fortune cookies']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick a cell from the clean data to test and look at it\n",
    "test_text = [test['text_no_punctuation'][8]]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the encoder's features for the decoder\n",
    "\n",
    "tok1.fit_on_texts(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize test text\n",
    "\n",
    "raw_tokenized = tok1.texts_to_sequences(test_text)\n",
    "raw_tokenized = tf.keras.preprocessing.sequence.pad_sequences(raw_tokenized, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the encoder state of the new sentence\n",
    "body_encoding = encoder_model.predict(raw_tokenized) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get output shapes of decoder word embedding\n",
    "latent_dim = seq2seq_Model.get_layer('Decoder-Word-Embedding').output_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get layer method for getting the embedding (word clusters)\n",
    "\n",
    "decoder_inputs = seq2seq_Model.get_layer('Decoder-Input').input \n",
    "dec_emb = seq2seq_Model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "dec_bn = seq2seq_Model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "gru_inference_state_input = tf.keras.Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n",
    "gru_out, gru_state_out = seq2seq_Model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
    "\n",
    "# Reconstruct dense layers\n",
    "dec_bn2 = seq2seq_Model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "dense_out = seq2seq_Model.get_layer('Final-Output-Dense')(dec_bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model = tf.keras.Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the encoder's embedding before its updated by decoder for later\n",
    "\n",
    "original_body_encoding = body_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value = np.array(tok2.word_index['_start_']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentence = []\n",
    "stop_condition = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_inv = dict((v, k) for k, v in tok2.word_index.items())\n",
    "#vocabulary_inv[0] = \"<PAD/>\"\n",
    "#vocabulary_inv[1] = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '_start_',\n",
       " 2: '_end_',\n",
       " 3: 'great',\n",
       " 4: 'the',\n",
       " 5: 'good',\n",
       " 6: 'for',\n",
       " 7: 'a',\n",
       " 8: 'and',\n",
       " 9: 'best',\n",
       " 10: 'not',\n",
       " 11: 'my',\n",
       " 12: 'love',\n",
       " 13: 'this',\n",
       " 14: 'it',\n",
       " 15: 'coffee',\n",
       " 16: 'but',\n",
       " 17: 'i',\n",
       " 18: 'of',\n",
       " 19: 'tea',\n",
       " 20: 'to',\n",
       " 21: 'is',\n",
       " 22: 'taste',\n",
       " 23: 'in',\n",
       " 24: 'these',\n",
       " 25: 'like',\n",
       " 26: 'product',\n",
       " 27: 'dog',\n",
       " 28: 'very',\n",
       " 29: 'price',\n",
       " 30: 'flavor',\n",
       " 31: 'ever',\n",
       " 32: 'food',\n",
       " 33: 'delicious',\n",
       " 34: 'as',\n",
       " 35: 'you',\n",
       " 36: 'loves',\n",
       " 37: 'with',\n",
       " 38: 'favorite',\n",
       " 39: 'too',\n",
       " 40: 'are',\n",
       " 41: 'dogs',\n",
       " 42: 'on',\n",
       " 43: 'tasty',\n",
       " 44: 'excellent',\n",
       " 45: 'better',\n",
       " 46: 'so',\n",
       " 47: 'snack',\n",
       " 48: 'healthy',\n",
       " 49: 'them',\n",
       " 50: 'just',\n",
       " 51: 'chocolate',\n",
       " 52: 'than',\n",
       " 53: 'what',\n",
       " 54: 'at',\n",
       " 55: 'free',\n",
       " 56: 'no',\n",
       " 57: 'one',\n",
       " 58: 'yummy',\n",
       " 59: 'tastes',\n",
       " 60: 'hot',\n",
       " 61: 'tasting',\n",
       " 62: 'stuff',\n",
       " 63: 'all',\n",
       " 64: 'chips',\n",
       " 65: 'cats',\n",
       " 66: 'cat',\n",
       " 67: 'really',\n",
       " 68: 'nice',\n",
       " 69: 'treat',\n",
       " 70: 'sweet',\n",
       " 71: 'have',\n",
       " 72: 'its',\n",
       " 73: 'little',\n",
       " 74: 'me',\n",
       " 75: 'buy',\n",
       " 76: 'quality',\n",
       " 77: 'perfect',\n",
       " 78: 'your',\n",
       " 79: 'treats',\n",
       " 80: 'from',\n",
       " 81: 'be',\n",
       " 82: 'dont',\n",
       " 83: 'sugar',\n",
       " 84: 'green',\n",
       " 85: 'mix',\n",
       " 86: 'organic',\n",
       " 87: 'if',\n",
       " 88: 'easy',\n",
       " 89: 'much',\n",
       " 90: 'cup',\n",
       " 91: 'bad',\n",
       " 92: 'cookies',\n",
       " 93: 'that',\n",
       " 94: 'was',\n",
       " 95: 'more',\n",
       " 96: 'they',\n",
       " 97: 'gluten',\n",
       " 98: 'way',\n",
       " 99: 'wonderful',\n",
       " 100: 'only',\n",
       " 101: 'out',\n",
       " 102: 'awesome',\n",
       " 103: 'strong',\n",
       " 104: 'can',\n",
       " 105: 'get',\n",
       " 106: 'amazon',\n",
       " 107: 'candy',\n",
       " 108: 'salt',\n",
       " 109: 'value',\n",
       " 110: 'our',\n",
       " 111: 'cant',\n",
       " 112: 'or',\n",
       " 113: 'drink',\n",
       " 114: 'sauce',\n",
       " 115: 'cereal',\n",
       " 116: 'bars',\n",
       " 117: 'ive',\n",
       " 118: 'an',\n",
       " 119: 'fresh',\n",
       " 120: 'oil',\n",
       " 121: 'eat',\n",
       " 122: 'up',\n",
       " 123: 'real',\n",
       " 124: 'made',\n",
       " 125: 'bar',\n",
       " 126: 'will',\n",
       " 127: 'hard',\n",
       " 128: 'find',\n",
       " 129: 'do',\n",
       " 130: 'had',\n",
       " 131: 'ok',\n",
       " 132: 'coconut',\n",
       " 133: 'go',\n",
       " 134: 'baby',\n",
       " 135: 'decaf',\n",
       " 136: 'kcup',\n",
       " 137: 'low',\n",
       " 138: 'time',\n",
       " 139: 'bold',\n",
       " 140: 'water',\n",
       " 141: 'kids',\n",
       " 142: 'popcorn',\n",
       " 143: 'breakfast',\n",
       " 144: 'happy',\n",
       " 145: 'high',\n",
       " 146: 'there',\n",
       " 147: 'new',\n",
       " 148: 'dark',\n",
       " 149: 'butter',\n",
       " 150: 'expensive',\n",
       " 151: 'worth',\n",
       " 152: 'deal',\n",
       " 153: 'blend',\n",
       " 154: 'pretty',\n",
       " 155: 'amazing',\n",
       " 156: 'yum',\n",
       " 157: 'works',\n",
       " 158: 'expected',\n",
       " 159: 'use',\n",
       " 160: 'vanilla',\n",
       " 161: 'old',\n",
       " 162: 'by',\n",
       " 163: 'makes',\n",
       " 164: 'natural',\n",
       " 165: 'loved',\n",
       " 166: 'rice',\n",
       " 167: 'kcups',\n",
       " 168: 'money',\n",
       " 169: 'has',\n",
       " 170: 'gift',\n",
       " 171: 'shipping',\n",
       " 172: 'chicken',\n",
       " 173: 'again',\n",
       " 174: 'service',\n",
       " 175: 'doesnt',\n",
       " 176: 'smooth',\n",
       " 177: 'we',\n",
       " 178: 'alternative',\n",
       " 179: 'big',\n",
       " 180: 'fast',\n",
       " 181: 'soup',\n",
       " 182: 'peanut',\n",
       " 183: 'jerky',\n",
       " 184: 'does',\n",
       " 185: 'roast',\n",
       " 186: 'cookie',\n",
       " 187: 'pack',\n",
       " 188: 'light',\n",
       " 189: 'protein',\n",
       " 190: 'bread',\n",
       " 191: 'small',\n",
       " 192: 'quick',\n",
       " 193: 'without',\n",
       " 194: 'black',\n",
       " 195: 'well',\n",
       " 196: 'some',\n",
       " 197: 'even',\n",
       " 198: 'pasta',\n",
       " 199: 'review',\n",
       " 200: 'would',\n",
       " 201: 'far',\n",
       " 202: 'milk',\n",
       " 203: 'instant',\n",
       " 204: 'other',\n",
       " 205: 'packaging',\n",
       " 206: 'k',\n",
       " 207: 'chai',\n",
       " 208: 'item',\n",
       " 209: 'found',\n",
       " 210: 'flavored',\n",
       " 211: 'didnt',\n",
       " 212: 'make',\n",
       " 213: 'fantastic',\n",
       " 214: 'syrup',\n",
       " 215: 'enough',\n",
       " 216: 'bit',\n",
       " 217: 'convenient',\n",
       " 218: 'energy',\n",
       " 219: 'fruit',\n",
       " 220: 'cocoa',\n",
       " 221: 'right',\n",
       " 222: 'thing',\n",
       " 223: 'spicy',\n",
       " 224: 'beans',\n",
       " 225: 'dry',\n",
       " 226: 'ginger',\n",
       " 227: 'bag',\n",
       " 228: 'cheese',\n",
       " 229: 'about',\n",
       " 230: 'salty',\n",
       " 231: 'work',\n",
       " 232: 'when',\n",
       " 233: 'super',\n",
       " 234: 'chip',\n",
       " 235: 'now',\n",
       " 236: 'diet',\n",
       " 237: 'spice',\n",
       " 238: 'rich',\n",
       " 239: 'bags',\n",
       " 240: '5',\n",
       " 241: 'order',\n",
       " 242: 'crunchy',\n",
       " 243: 'whole',\n",
       " 244: 'keurig',\n",
       " 245: 'red',\n",
       " 246: 'were',\n",
       " 247: 'gum',\n",
       " 248: 'honey',\n",
       " 249: 'likes',\n",
       " 250: 'most',\n",
       " 251: 'never',\n",
       " 252: 'beef',\n",
       " 253: 'must',\n",
       " 254: 'first',\n",
       " 255: 'wow',\n",
       " 256: 'kind',\n",
       " 257: 'box',\n",
       " 258: 'weak',\n",
       " 259: 'same',\n",
       " 260: 'tasted',\n",
       " 261: 'size',\n",
       " 262: '2',\n",
       " 263: 'could',\n",
       " 264: 'still',\n",
       " 265: 'cups',\n",
       " 266: 'oatmeal',\n",
       " 267: 'nothing',\n",
       " 268: 'fat',\n",
       " 269: 'flavors',\n",
       " 270: 'potato',\n",
       " 271: 'day',\n",
       " 272: 'try',\n",
       " 273: 'long',\n",
       " 274: 'brand',\n",
       " 275: 'market',\n",
       " 276: 'tried',\n",
       " 277: 'morning',\n",
       " 278: 'bitter',\n",
       " 279: 'around',\n",
       " 280: 'every',\n",
       " 281: 'used',\n",
       " 282: 'nuts',\n",
       " 283: 'lovers',\n",
       " 284: 'cheaper',\n",
       " 285: 'training',\n",
       " 286: 'store',\n",
       " 287: 'less',\n",
       " 288: 'over',\n",
       " 289: 'im',\n",
       " 290: 'did',\n",
       " 291: 'crackers',\n",
       " 292: 'french',\n",
       " 293: 'regular',\n",
       " 294: 'mountain',\n",
       " 295: 'white',\n",
       " 296: 'got',\n",
       " 297: 'wrong',\n",
       " 298: 'puppy',\n",
       " 299: 'starbucks',\n",
       " 300: 'always',\n",
       " 301: 'flavorful',\n",
       " 302: 'after',\n",
       " 303: 'china',\n",
       " 304: 'back',\n",
       " 305: 'why',\n",
       " 306: 'corn',\n",
       " 307: 'world',\n",
       " 308: 'wont',\n",
       " 309: 'lemon',\n",
       " 310: 'substitute',\n",
       " 311: 'pricey',\n",
       " 312: 'stale',\n",
       " 313: 'granola',\n",
       " 314: 'any',\n",
       " 315: 'delivery',\n",
       " 316: 'fun',\n",
       " 317: 'choice',\n",
       " 318: 'cinnamon',\n",
       " 319: 'horrible',\n",
       " 320: 'beware',\n",
       " 321: 'gf',\n",
       " 322: 'picky',\n",
       " 323: 'another',\n",
       " 324: 'need',\n",
       " 325: 'fine',\n",
       " 326: 'powder',\n",
       " 327: 'ingredients',\n",
       " 328: 'iced',\n",
       " 329: 'meal',\n",
       " 330: 'different',\n",
       " 331: 'em',\n",
       " 332: 'full',\n",
       " 333: 'poor',\n",
       " 334: 'decent',\n",
       " 335: 'variety',\n",
       " 336: 'how',\n",
       " 337: 'greenies',\n",
       " 338: 'texture',\n",
       " 339: 'finally',\n",
       " 340: 'worst',\n",
       " 341: 'house',\n",
       " 342: 'seasoning',\n",
       " 343: 'stash',\n",
       " 344: 'terrible',\n",
       " 345: 'formula',\n",
       " 346: 'glutenfree',\n",
       " 347: 'espresso',\n",
       " 348: 'soft',\n",
       " 349: 'down',\n",
       " 350: 'should',\n",
       " 351: 'licorice',\n",
       " 352: 'pods',\n",
       " 353: 'refreshing',\n",
       " 354: 'who',\n",
       " 355: 'simply',\n",
       " 356: 'home',\n",
       " 357: 'smells',\n",
       " 358: 'last',\n",
       " 359: 'hazelnut',\n",
       " 360: 'cans',\n",
       " 361: 'waste',\n",
       " 362: 'stars',\n",
       " 363: 'cake',\n",
       " 364: 'smell',\n",
       " 365: 'canned',\n",
       " 366: 'almonds',\n",
       " 367: 'yet',\n",
       " 368: 'okay',\n",
       " 369: 'available',\n",
       " 370: 'juice',\n",
       " 371: 'blue',\n",
       " 372: 'pop',\n",
       " 373: 'here',\n",
       " 374: 'pretzels',\n",
       " 375: 'health',\n",
       " 376: 'exactly',\n",
       " 377: 'crazy',\n",
       " 378: 'cream',\n",
       " 379: 'original',\n",
       " 380: 'family',\n",
       " 381: 'many',\n",
       " 382: 'almost',\n",
       " 383: 'extra',\n",
       " 384: 'crunch',\n",
       " 385: 'chews',\n",
       " 386: 'purchase',\n",
       " 387: 'want',\n",
       " 388: 'olive',\n",
       " 389: 'absolutely',\n",
       " 390: 'son',\n",
       " 391: 'wheat',\n",
       " 392: 'say',\n",
       " 393: 'sure',\n",
       " 394: '1',\n",
       " 395: 'nutritious',\n",
       " 396: '3',\n",
       " 397: 'disappointed',\n",
       " 398: 'off',\n",
       " 399: 'dried',\n",
       " 400: 'lot',\n",
       " 401: 'mild',\n",
       " 402: 'senseo',\n",
       " 403: 'oh',\n",
       " 404: 'liked',\n",
       " 405: 'date',\n",
       " 406: 'addictive',\n",
       " 407: 'apple',\n",
       " 408: 'chew',\n",
       " 409: 'know',\n",
       " 410: 'own',\n",
       " 411: '4',\n",
       " 412: 'snacks',\n",
       " 413: 'overpriced',\n",
       " 414: 'recommended',\n",
       " 415: 'grey',\n",
       " 416: 'eating',\n",
       " 417: 'pure',\n",
       " 418: 'bought',\n",
       " 419: 'earl',\n",
       " 420: 'wish',\n",
       " 421: 'hit',\n",
       " 422: 'calories',\n",
       " 423: 'quite',\n",
       " 424: 'nut',\n",
       " 425: 'says',\n",
       " 426: 'people',\n",
       " 427: 'flour',\n",
       " 428: 'brown',\n",
       " 429: 'baking',\n",
       " 430: 'think',\n",
       " 431: 'cheap',\n",
       " 432: 'heaven',\n",
       " 433: 'looking',\n",
       " 434: 'life',\n",
       " 435: 'option',\n",
       " 436: 'package',\n",
       " 437: 'things',\n",
       " 438: 'please',\n",
       " 439: 'idea',\n",
       " 440: 'almond',\n",
       " 441: 'special',\n",
       " 442: 'lots',\n",
       " 443: 'ordered',\n",
       " 444: 'filling',\n",
       " 445: 'two',\n",
       " 446: 'give',\n",
       " 447: 'chewy',\n",
       " 448: 'chili',\n",
       " 449: 'been',\n",
       " 450: 'customer',\n",
       " 451: 'calorie',\n",
       " 452: 'soda',\n",
       " 453: 'toy',\n",
       " 454: 'am',\n",
       " 455: 'bbq',\n",
       " 456: 'seeds',\n",
       " 457: 'ice',\n",
       " 458: 'orange',\n",
       " 459: 'carb',\n",
       " 460: 'vinegar',\n",
       " 461: 'arrived',\n",
       " 462: 'sour',\n",
       " 463: 'mint',\n",
       " 464: 'products',\n",
       " 465: 'needs',\n",
       " 466: 'herbal',\n",
       " 467: 'fan',\n",
       " 468: 'oz',\n",
       " 469: 'bean',\n",
       " 470: 'those',\n",
       " 471: 'yes',\n",
       " 472: 'soy',\n",
       " 473: 'pumpkin',\n",
       " 474: 'awful',\n",
       " 475: 'company',\n",
       " 476: 'favorites',\n",
       " 477: 'italian',\n",
       " 478: 'fabulous',\n",
       " 479: 'pepper',\n",
       " 480: 'save',\n",
       " 481: 'noodles',\n",
       " 482: '6',\n",
       " 483: 'something',\n",
       " 484: 'wellness',\n",
       " 485: 'dressing',\n",
       " 486: 'maple',\n",
       " 487: '12',\n",
       " 488: 'raw',\n",
       " 489: 'us',\n",
       " 490: 'weight',\n",
       " 491: 'teeth',\n",
       " 492: 'highly',\n",
       " 493: 'raspberry',\n",
       " 494: 'their',\n",
       " 495: '100',\n",
       " 496: 'brew',\n",
       " 497: 'cold',\n",
       " 498: 'received',\n",
       " 499: 'name',\n",
       " 500: 'aroma',\n",
       " 501: 'salmon',\n",
       " 502: 'advertised',\n",
       " 503: 'top',\n",
       " 504: 'kitty',\n",
       " 505: 'grain',\n",
       " 506: 'fiber',\n",
       " 507: 'thought',\n",
       " 508: 'wanted',\n",
       " 509: 'baked',\n",
       " 510: 'teas',\n",
       " 511: 'newmans',\n",
       " 512: 'daughter',\n",
       " 513: 'stores',\n",
       " 514: 'goodness',\n",
       " 515: 'helps',\n",
       " 516: 'though',\n",
       " 517: 'jelly',\n",
       " 518: 'priced',\n",
       " 519: 'cherry',\n",
       " 520: 'everything',\n",
       " 521: 'gourmet',\n",
       " 522: 'before',\n",
       " 523: 'definitely',\n",
       " 524: 'close',\n",
       " 525: 'bottle',\n",
       " 526: 'tuna',\n",
       " 527: 'where',\n",
       " 528: 'cooking',\n",
       " 529: 'creamy',\n",
       " 530: 'fish',\n",
       " 531: 'star',\n",
       " 532: 'mustard',\n",
       " 533: 'away',\n",
       " 534: 'sea',\n",
       " 535: 'goes',\n",
       " 536: 'simple',\n",
       " 537: 'keeps',\n",
       " 538: 'artificial',\n",
       " 539: 'years',\n",
       " 540: 'husband',\n",
       " 541: '10',\n",
       " 542: 'plus',\n",
       " 543: 'recipe',\n",
       " 544: 'buying',\n",
       " 545: 'greatest',\n",
       " 546: 'keep',\n",
       " 547: 'salad',\n",
       " 548: 'premium',\n",
       " 549: 'sticks',\n",
       " 550: 'else',\n",
       " 551: 'bland',\n",
       " 552: 'large',\n",
       " 553: 'cost',\n",
       " 554: 'mini',\n",
       " 555: 'english',\n",
       " 556: 'broken',\n",
       " 557: 'peach',\n",
       " 558: 'change',\n",
       " 559: 'meat',\n",
       " 560: 'nom',\n",
       " 561: 'live',\n",
       " 562: 'anything',\n",
       " 563: 'replacement',\n",
       " 564: 'medium',\n",
       " 565: 'gone',\n",
       " 566: 'hair',\n",
       " 567: 'kick',\n",
       " 568: 'cracker',\n",
       " 569: 'stevia',\n",
       " 570: 'expiration',\n",
       " 571: 'add',\n",
       " 572: 'sodium',\n",
       " 573: 'bones',\n",
       " 574: 'clean',\n",
       " 575: 'caramel',\n",
       " 576: 'rocks',\n",
       " 577: 'hate',\n",
       " 578: 'foods',\n",
       " 579: 'earth',\n",
       " 580: 'satisfying',\n",
       " 581: 'cappuccino',\n",
       " 582: 'year',\n",
       " 583: 'wild',\n",
       " 584: 'true',\n",
       " 585: 'allergies',\n",
       " 586: 'bulk',\n",
       " 587: 'ones',\n",
       " 588: 'kid',\n",
       " 589: 'believe',\n",
       " 590: 'half',\n",
       " 591: 'everyone',\n",
       " 592: 'needed',\n",
       " 593: 'making',\n",
       " 594: 'maybe',\n",
       " 595: 'then',\n",
       " 596: 'fuel',\n",
       " 597: 'magic',\n",
       " 598: 'glad',\n",
       " 599: 'actually',\n",
       " 600: 'youll',\n",
       " 601: 'lime',\n",
       " 602: 'mom',\n",
       " 603: 'contains',\n",
       " 604: 'wife',\n",
       " 605: 'leaf',\n",
       " 606: 'wouldnt',\n",
       " 607: 'thats',\n",
       " 608: 'ingredient',\n",
       " 609: 'sweetener',\n",
       " 610: 'changed',\n",
       " 611: 'care',\n",
       " 612: 'usa',\n",
       " 613: 'kettle',\n",
       " 614: 'bite',\n",
       " 615: 'pill',\n",
       " 616: 'nutrition',\n",
       " 617: 'dented',\n",
       " 618: 'caffeine',\n",
       " 619: 'seems',\n",
       " 620: 'popchips',\n",
       " 621: 'thank',\n",
       " 622: 'terrific',\n",
       " 623: 'pleasant',\n",
       " 624: 'pleased',\n",
       " 625: 'smaller',\n",
       " 626: 'enjoy',\n",
       " 627: 'beautiful',\n",
       " 628: 'homemade',\n",
       " 629: 'lover',\n",
       " 630: 'addicted',\n",
       " 631: 'aftertaste',\n",
       " 632: 'whats',\n",
       " 633: 'vegan',\n",
       " 634: 'packs',\n",
       " 635: 'take',\n",
       " 636: 'added',\n",
       " 637: 'pet',\n",
       " 638: 'unique',\n",
       " 639: 'version',\n",
       " 640: 'seller',\n",
       " 641: 'look',\n",
       " 642: 'miss',\n",
       " 643: 'jet',\n",
       " 644: 'theyre',\n",
       " 645: 'stop',\n",
       " 646: 'seed',\n",
       " 647: 'pie',\n",
       " 648: 'help',\n",
       " 649: 'touch',\n",
       " 650: 'grocery',\n",
       " 651: 'job',\n",
       " 652: 'movie',\n",
       " 653: 'may',\n",
       " 654: 'nasty',\n",
       " 655: 'double',\n",
       " 656: 'local',\n",
       " 657: 'delight',\n",
       " 658: 'wine',\n",
       " 659: 'per',\n",
       " 660: 'zukes',\n",
       " 661: 'outstanding',\n",
       " 662: 'plain',\n",
       " 663: 'warning',\n",
       " 664: 'blueberry',\n",
       " 665: 'garlic',\n",
       " 666: 'looks',\n",
       " 667: 'summer',\n",
       " 668: 'peppermint',\n",
       " 669: 'rock',\n",
       " 670: 'misleading',\n",
       " 671: 'gummy',\n",
       " 672: 'going',\n",
       " 673: 'pieces',\n",
       " 674: 'lunch',\n",
       " 675: 'bacon',\n",
       " 676: 'thin',\n",
       " 677: 'source',\n",
       " 678: 'crispy',\n",
       " 679: 'plastic',\n",
       " 680: 'feel',\n",
       " 681: 'winner',\n",
       " 682: 'dental',\n",
       " 683: 'ground',\n",
       " 684: 'healthier',\n",
       " 685: 'she',\n",
       " 686: 'peanuts',\n",
       " 687: 'gets',\n",
       " 688: 'read',\n",
       " 689: 'isnt',\n",
       " 690: 'toddler',\n",
       " 691: 'three',\n",
       " 692: 'description',\n",
       " 693: 'described',\n",
       " 694: 'youre',\n",
       " 695: 'puck',\n",
       " 696: 'couldnt',\n",
       " 697: 'reviews',\n",
       " 698: 'tullys',\n",
       " 699: 'wheres',\n",
       " 700: 'sugarfree',\n",
       " 701: 'bears',\n",
       " 702: 'lovely',\n",
       " 703: 'machine',\n",
       " 704: 'anywhere',\n",
       " 705: 'tiny',\n",
       " 706: 'boxes',\n",
       " 707: 'coffees',\n",
       " 708: 'others',\n",
       " 709: 'doggie',\n",
       " 710: 'golden',\n",
       " 711: 'classic',\n",
       " 712: 'mouth',\n",
       " 713: 'come',\n",
       " 714: 'picture',\n",
       " 715: 'huge',\n",
       " 716: 'gross',\n",
       " 717: 'lipton',\n",
       " 718: 'start',\n",
       " 719: 'tummy',\n",
       " 720: 'hands',\n",
       " 721: 'pizza',\n",
       " 722: 'bargain',\n",
       " 723: 'grove',\n",
       " 724: 'banana',\n",
       " 725: 'bone',\n",
       " 726: 'month',\n",
       " 727: 'kit',\n",
       " 728: 'eater',\n",
       " 729: 'her',\n",
       " 730: 'omg',\n",
       " 731: 'strawberry',\n",
       " 732: 'party',\n",
       " 733: 'everyday',\n",
       " 734: 'yuck',\n",
       " 735: 'gloria',\n",
       " 736: 'babies',\n",
       " 737: 'pay',\n",
       " 738: 'wolfgang',\n",
       " 739: 'heat',\n",
       " 740: 'n',\n",
       " 741: 'mill',\n",
       " 742: 'pockets',\n",
       " 743: 'pancake',\n",
       " 744: 'biscuits',\n",
       " 745: 'solid',\n",
       " 746: 'extract',\n",
       " 747: '8',\n",
       " 748: 'gold',\n",
       " 749: 'curry',\n",
       " 750: 'tough',\n",
       " 751: 'mocha',\n",
       " 752: 'totally',\n",
       " 753: 'cider',\n",
       " 754: 'latte',\n",
       " 755: 'control',\n",
       " 756: 'steak',\n",
       " 757: 'square',\n",
       " 758: 'kona',\n",
       " 759: 'dessert',\n",
       " 760: 'bobs',\n",
       " 761: 'addition',\n",
       " 762: 'problem',\n",
       " 763: 'watch',\n",
       " 764: 'roasted',\n",
       " 765: 'sick',\n",
       " 766: 'berry',\n",
       " 767: 'mints',\n",
       " 768: 'stomach',\n",
       " 769: 'dr',\n",
       " 770: 'reasonable',\n",
       " 771: 'recommend',\n",
       " 772: 'tart',\n",
       " 773: 'few',\n",
       " 774: 'single',\n",
       " 775: 'prefer',\n",
       " 776: 'into',\n",
       " 777: 'difference',\n",
       " 778: 'pod',\n",
       " 779: 'staple',\n",
       " 780: 'instead',\n",
       " 781: 'mango',\n",
       " 782: 'slim',\n",
       " 783: 'dinner',\n",
       " 784: 'handy',\n",
       " 785: 'mac',\n",
       " 786: 'sent',\n",
       " 787: 'o',\n",
       " 788: 'absolute',\n",
       " 789: 'remember',\n",
       " 790: 'count',\n",
       " 791: 'average',\n",
       " 792: 'jack',\n",
       " 793: 'cut',\n",
       " 794: 'worked',\n",
       " 795: 'pancakes',\n",
       " 796: 'bran',\n",
       " 797: 'side',\n",
       " 798: 'days',\n",
       " 799: 'getting',\n",
       " 800: 'noodle',\n",
       " 801: 'second',\n",
       " 802: 'tomato',\n",
       " 803: 'truly',\n",
       " 804: 'crack',\n",
       " 805: 'quaker',\n",
       " 806: 'joe',\n",
       " 807: 'jeans',\n",
       " 808: 'came',\n",
       " 809: 'diamond',\n",
       " 810: 'nutiva',\n",
       " 811: 'pork',\n",
       " 812: 'packaged',\n",
       " 813: 'plant',\n",
       " 814: 'cute',\n",
       " 815: 'paste',\n",
       " 816: 'sensitive',\n",
       " 817: 'convenience',\n",
       " 818: 'brands',\n",
       " 819: 'crystal',\n",
       " 820: 'prices',\n",
       " 821: 'style',\n",
       " 822: 'gas',\n",
       " 823: 'oats',\n",
       " 824: 'bay',\n",
       " 825: 'affordable',\n",
       " 826: 'drinking',\n",
       " 827: 'amount',\n",
       " 828: 'cherries',\n",
       " 829: 'versatile',\n",
       " 830: 'crisp',\n",
       " 831: 'expect',\n",
       " 832: 'loose',\n",
       " 833: 'gotta',\n",
       " 834: 'longer',\n",
       " 835: 'jalapeno',\n",
       " 836: 'veggie',\n",
       " 837: 'planet',\n",
       " 838: 'cafe',\n",
       " 839: 'turkey',\n",
       " 840: 'jar',\n",
       " 841: 'tree',\n",
       " 842: 'boost',\n",
       " 843: 'smart',\n",
       " 844: 'fair',\n",
       " 845: 'splenda',\n",
       " 846: 'belly',\n",
       " 847: 'creme',\n",
       " 848: 'twinings',\n",
       " 849: 'skin',\n",
       " 850: 'increase',\n",
       " 851: 'thanks',\n",
       " 852: 'calm',\n",
       " 853: 'disappointing',\n",
       " 854: 'timothys',\n",
       " 855: 'darn',\n",
       " 856: 'short',\n",
       " 857: 'mixed',\n",
       " 858: 'mess',\n",
       " 859: 'being',\n",
       " 860: 'both',\n",
       " 861: 'subscribe',\n",
       " 862: 'liver',\n",
       " 863: 'open',\n",
       " 864: 'emerils',\n",
       " 865: 'shipped',\n",
       " 866: 'surprise',\n",
       " 867: 'pamelas',\n",
       " 868: 'tangy',\n",
       " 869: 'experience',\n",
       " 870: 'liquid',\n",
       " 871: 'finicky',\n",
       " 872: 'drinks',\n",
       " 873: 'amazoncom',\n",
       " 874: 'effective',\n",
       " 875: 'nutty',\n",
       " 876: 'coffe',\n",
       " 877: 'slightly',\n",
       " 878: 'salsa',\n",
       " 879: 'daily',\n",
       " 880: 'truffle',\n",
       " 881: 'months',\n",
       " 882: 'five',\n",
       " 883: 'toffee',\n",
       " 884: 'night',\n",
       " 885: 'he',\n",
       " 886: 'once',\n",
       " 887: 'beat',\n",
       " 888: 'tortilla',\n",
       " 889: '50',\n",
       " 890: 'christmas',\n",
       " 891: 'also',\n",
       " 892: 'beer',\n",
       " 893: 'fix',\n",
       " 894: 'safe',\n",
       " 895: 'stinky',\n",
       " 896: 'gave',\n",
       " 897: 'fruity',\n",
       " 898: 'stick',\n",
       " 899: 'chocolates',\n",
       " 900: 'chewers',\n",
       " 901: 'msg',\n",
       " 902: 'san',\n",
       " 903: 'leaves',\n",
       " 904: 'pictured',\n",
       " 905: 'cook',\n",
       " 906: 'office',\n",
       " 907: 'delish',\n",
       " 908: 'hoped',\n",
       " 909: 'authentic',\n",
       " 910: 'kitchen',\n",
       " 911: 'lavazza',\n",
       " 912: 'thai',\n",
       " 913: 'wake',\n",
       " 914: 'cheddar',\n",
       " 915: 'kitties',\n",
       " 916: 'friend',\n",
       " 917: 'problems',\n",
       " 918: 'weird',\n",
       " 919: 'extremely',\n",
       " 920: 'id',\n",
       " 921: 'maker',\n",
       " 922: 'tell',\n",
       " 923: 'quantity',\n",
       " 924: 'watery',\n",
       " 925: 'beverage',\n",
       " 926: 'hint',\n",
       " 927: 'purchased',\n",
       " 928: 'older',\n",
       " 929: 'ounce',\n",
       " 930: 'sampler',\n",
       " 931: 'least',\n",
       " 932: 'robust',\n",
       " 933: 'disgusting',\n",
       " 934: 'friendly',\n",
       " 935: 'quickly',\n",
       " 936: 'power',\n",
       " 937: 'grass',\n",
       " 938: 'eats',\n",
       " 939: 'hips',\n",
       " 940: 'messy',\n",
       " 941: 'shop',\n",
       " 942: 'busy',\n",
       " 943: 'pecan',\n",
       " 944: 'wait',\n",
       " 945: 'color',\n",
       " 946: 'online',\n",
       " 947: 'country',\n",
       " 948: 'gummi',\n",
       " 949: 'donut',\n",
       " 950: 'wasnt',\n",
       " 951: 'jasmine',\n",
       " 952: 'breath',\n",
       " 953: 'said',\n",
       " 954: 'impressed',\n",
       " 955: 'while',\n",
       " 956: 'candies',\n",
       " 957: 'flakes',\n",
       " 958: 'packages',\n",
       " 959: 'sesame',\n",
       " 960: 'thick',\n",
       " 961: 'seem',\n",
       " 962: 'his',\n",
       " 963: 'delivered',\n",
       " 964: 'bed',\n",
       " 965: 'paws',\n",
       " 966: 'gravy',\n",
       " 967: 'ordering',\n",
       " 968: 'container',\n",
       " 969: 'eaten',\n",
       " 970: 'melted',\n",
       " 971: 'expired',\n",
       " 972: 'anyone',\n",
       " 973: 'weve',\n",
       " 974: 'addicting',\n",
       " 975: 'check',\n",
       " 976: 'missing',\n",
       " 977: 'nestle',\n",
       " 978: 'acid',\n",
       " 979: 'cool',\n",
       " 980: 'see',\n",
       " 981: 'virgin',\n",
       " 982: 'gives',\n",
       " 983: 'delightful',\n",
       " 984: 'put',\n",
       " 985: 'zero',\n",
       " 986: 'bring',\n",
       " 987: 'superb',\n",
       " 988: 'kidding',\n",
       " 989: 'ramen',\n",
       " 990: 'yogurt',\n",
       " 991: '24',\n",
       " 992: 'dairy',\n",
       " 993: 'lasting',\n",
       " 994: 'cal',\n",
       " 995: 'careful',\n",
       " 996: 'cranberry',\n",
       " 997: 'rip',\n",
       " 998: 'links',\n",
       " 999: 'body',\n",
       " 1000: 'lamb',\n",
       " ...}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "cookie\n",
      "but\n",
      "_end_\n"
     ]
    }
   ],
   "source": [
    "while not stop_condition:\n",
    "    #print(1)\n",
    "    preds, st = decoder_model.predict([state_value, body_encoding])\n",
    "\n",
    "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "    pred_word_str = vocabulary_inv[pred_idx]\n",
    "    print(pred_word_str)\n",
    "    if pred_word_str == '_end_' or len(decoded_sentence) >= maxlen2:\n",
    "        stop_condition = True\n",
    "        break\n",
    "    decoded_sentence.append(pred_word_str)\n",
    "\n",
    "    # update the decoder for the next word\n",
    "    body_encoding = st\n",
    "    state_value = np.array(pred_idx).reshape(1, 1)\n",
    "    #print(state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_start_ fast food fortune cookie _end_']\n"
     ]
    }
   ],
   "source": [
    "#compare to original summary\n",
    "\n",
    "print([test['summary_no_punctuation'][8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
